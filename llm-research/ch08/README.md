## Model compression

### LLM Compression Training and Inference
#### Pruning
#### Sparsity
#### Quantization
#### Knowledge Distillation
Knowledge distillation is a machine learning technique that involves transferring knowledge from a large, complex model (the "teacher" model) to a smaller, more efficient model (the "student" model). The goal is to enable the student model to achieve similar performance to the teacher model while being more compact and faster to run, which is particularly useful for applications with resource constraints, such as mobile devices or embedded systems.


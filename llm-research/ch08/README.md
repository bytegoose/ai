## Model compression

### LLM Compression Training and Inference
#### Pruning
#### Sparsity
#### Quantization
#### Neural Architecture Search
#### Knowledge Distillation
Knowledge distillation is a machine learning technique that involves transferring knowledge from a large, complex model (the "teacher" model) to a smaller, more efficient model (the "student" model). The goal is to enable the student model to achieve similar performance to the teacher model while being more compact and faster to run, which is particularly useful for applications with resource constraints, such as mobile devices or embedded systems.

### On-Device Training


## References
1. BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Birdâ€™s-Eye View Representation
2. Lite Transformer with long-short range attention
3. Language Models are Few-Shot Learners
4. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
5. SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning
   Code: https://github.com/mit-han-lab/spatten-llm
6. llama.cpp
   Code: https://github.com/ggerganov/llama.cpp
8. tensor library for machine learning
   Code:https://github.com/ggerganov/ggml
9.TinyChatEngine on device LLM/VLM
   Code:https://github.com/mit-han-lab/TinyChatEngine/
10. AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION




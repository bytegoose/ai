### Transformer Reinforcement Learning

Fine-tuning and aligning transformer and diffusion models can be performed usingthe following methods:

- Supervised Fine-tuning (SFT)
- Reward Modeling (RM) 
- Proximal Policy Optimization (PPO) 
- Direct Preference Optimization (DPO)

Papers:
 1. Odds Ratio Preference Optimization (ORPO) by Jiwoo Hong, Noah Lee, and James Thorne (initial code xfactlab/orpo)
 2. Contrastive Preference Optimization (CPO). paper Contrastive Preference Optimization: Pushing the Boundaries of 
  LLM Performance in Machine Translation by Haoran Xu, and others.
 3. Proximal Policy Optimization (PPO). Paper https://arxiv.org/abs/1707.06347. Reference impl https://github.com/openai/summarize-from-feedback
 4. Direct Preference Optimization (DPO. Paper Direct Preference Optimization: Your Language Model is Secretly a Reward Model by Rafailov et al., 2023. 
 5. REINFORCE Leave-One-Out (RLOO). Paper Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
 6. Kahneman-Tversky Optimization (KTO). Paper KTO: Model Alignment as Prospect Theoretic Optimization by Kawin Ethayarajh, others


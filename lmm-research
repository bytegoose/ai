# LMM - Large Multimodal Models
## Modalities: text, audio, speech, video, image

### References
Microsoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)
VQA: Visual Question Answering (May 2015)
VideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)
LXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)
[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)
Unifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)
Flamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)
GIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)
BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)
Cross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)
KOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)
PaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)
LLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)
InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)
Towards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)
Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)
Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic (SenseTime, Jun 3, 2023)
Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)

# LMM - Large Multimodal Models
## Modalities: text, audio, speech, video, image

## Architectures (multilingual, audio, and vision capabilities)
BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models: https://arxiv.org/abs/2301.12597

GIT: A Generative Image-to-text Transformer for Vision and Language - https://arxiv.org/abs/2205.14100

Flamingo: a Visual Language Model for Few-Shot Learning. https://arxiv.org/abs/2204.14198

SpeechGPT: https://github.com/0nutation/SpeechGPT

Video Chat: https://github.com/mbzuai-oryx/Video-ChatGPT

PaLM-E: An Embodied Multimodal Language Model https://arxiv.org/abs/2303.03378. https://palm-e.github.io/

Multi Languages: https://x-llm.github.io/

PandaGPT (Voice, Image, Video): https://panda-gpt.github.io/

ChatBridge GPT (Voice , Image, Video): https://iva-chatbridge.github.io/

GPT4o - it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs

## Applications BioTech & Health Care
Pathology Assistant: https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology

Medical Visual Question Answering: https://github.com/xiaoman-zhang/PMC-VQA

Language and Vision Assistant for Medicine: https://github.com/microsoft/LLaVA-Med

### References
1. Microsoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)
2. VQA: Visual Question Answering (May 2015)
3. VideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)
LXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)
[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)
Unifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)
Flamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)
GIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)
BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)
Cross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)
KOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)
PaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)
LLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)
InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)
Towards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)
Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)
Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic (SenseTime, Jun 3, 2023)
Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)


1. Pagliardini, M., Paliotta, D., Jaggi, M., & Fleuret, F. (2023). Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention. Neural Information Processing Systems.
2. Li, J., Cotterell, R., & Sachan, M. (2021). Differentiable Subset Pruning of Transformer Heads. Transactions of the Association for Computational Linguistics, 9, 1442–1459.
3. Goyal, S., et al. (2020). PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination. International Conference on Machine Learning.
4. Wang, S., et al. (2020). Linformer: Self-Attention with Linear Complexity. ArXiv, abs/2006.04768.
5. Kitaev, N., et al. (2020). Reformer: The Efficient Transformer. ArXiv, abs/2001.04451.
6. Choromanski, K., et al. (2020). Rethinking Attention with Performers. ArXiv, abs/2009.14794.
7. Micikevicius, P., et al. (2017). Mixed Precision Training. ArXiv, abs/1710.03740.
8. Sun, X., et al. (2020). Ultra-Low Precision 4-bit Training of Deep Neural Networks. Neural Information Processing Systems.
9. Xiong, R., et al. (2020). On Layer Normalization in the Transformer Architecture. ArXiv, abs/2002.04745.
10. Chen, T., et al. (2016). Training Deep Nets with Sublinear Memory Cost. ArXiv, abs/1604.06174

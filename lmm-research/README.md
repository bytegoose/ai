# LMM - Large Multimodal Models
## Modalities: text, audio, speech, video, image

## Architectures (multilingual, audio, and vision capabilities)
BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models: https://arxiv.org/abs/2301.12597

GIT: A Generative Image-to-text Transformer for Vision and Language - https://arxiv.org/abs/2205.14100

Flamingo: a Visual Language Model for Few-Shot Learning. https://arxiv.org/abs/2204.14198

SpeechGPT: https://github.com/0nutation/SpeechGPT

Video Chat: https://github.com/mbzuai-oryx/Video-ChatGPT

PaLM-E: An Embodied Multimodal Language Model https://arxiv.org/abs/2303.03378. https://palm-e.github.io/

Multi Languages: https://x-llm.github.io/

PandaGPT (Voice, Image, Video): https://panda-gpt.github.io/

ChatBridge GPT (Voice , Image, Video): https://iva-chatbridge.github.io/

GPT4o - it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs

## Applications BioTech & Health Care
Pathology Assistant: https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology

Medical Visual Question Answering: https://github.com/xiaoman-zhang/PMC-VQA

Language and Vision Assistant for Medicine: https://github.com/microsoft/LLaVA-Med

### References
Microsoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)
VQA: Visual Question Answering (May 2015)
VideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)
LXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)
[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)
Unifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)
Flamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)
GIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)
BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)
Cross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)
KOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)
PaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)
LLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)
InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)
Towards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)
Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)
Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic (SenseTime, Jun 3, 2023)
Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)
